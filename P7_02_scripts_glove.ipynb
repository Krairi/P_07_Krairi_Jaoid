{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7 : Détectez les Bad Buzz grâce au Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement du contenu des tweets (pour modèle glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>date_du_poste</th>\n",
       "      <th>utilisateur</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 date_du_poste    utilisateur  \\\n",
       "0      0  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
       "1      0  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
       "2      0  Mon Apr 06 22:19:57 PDT 2009        ElleCTF   \n",
       "3      0  Mon Apr 06 22:19:57 PDT 2009         Karoli   \n",
       "4      0  Mon Apr 06 22:20:00 PDT 2009       joy_wolf   \n",
       "\n",
       "                                               tweet  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "df_dep = pd.read_csv('P7_03_fichiercsv_tweet.csv')\n",
    "df_dep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    799999\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dep.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep['polarite'] = df_dep['label'].map({\n",
    "    4 : 'positive',\n",
    "    0 : 'negative'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    800000\n",
       "negative    799999\n",
       "Name: polarite, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dep.polarite.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrait des données\n",
    "n_sample = 20000\n",
    "df_neg= df_dep[df_dep['polarite'] == 'negative'].sample(n=n_sample)\n",
    "df_pos = df_dep[df_dep['polarite'] == 'positive'].sample(n=n_sample)\n",
    "\n",
    "data = pd.concat([df_neg, df_pos], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    20000\n",
       "positive    20000\n",
       "Name: polarite, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.polarite.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
      "\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JK253\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JK253\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JK253\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#*****************************************\n",
    "# Préparation des librairie et des outils\n",
    "# pour le nettoyage des tweets\n",
    "#*****************************************\n",
    "\n",
    "# Expressions régulières\n",
    "import re\n",
    "\n",
    "# Pontuations\n",
    "import string\n",
    "ponctuations = list(string.punctuation)\n",
    "print(ponctuations)\n",
    "\n",
    "# Tokénisation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Lemmatisation\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Charger les stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "mots_vides = stopwords.words('english')\n",
    "print('\\n')\n",
    "print(mots_vides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer le contenu des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour nettoyage de chaque document\n",
    "# tweet = corps du tweet = document\n",
    "# ponctuations : liste des ponctuations\n",
    "# stopwords : liste des stopwords à retirer\n",
    "# lem : fonction pour la lemmatisation des termes\n",
    "\n",
    "def clean_tweet(tweet, ponctuations, stopwords, lem):\n",
    "    # Harmonisation de la casse\n",
    "    temp = tweet.lower()\n",
    "    # retier les contractions en anglais\n",
    "    temp = re.sub(\"'\", \"\", temp)\n",
    "    # retrait des @\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\", \"\", temp)\n",
    "    # retrait des #\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\", \"\", temp)\n",
    "    # retrait des liens web (http et https)\n",
    "    temp = re.sub(r'http\\S+','', temp)\n",
    "    # retrait des ponctuations\n",
    "    temp = \"\".join([char for char in list(temp) if not (char in ponctuations)])\n",
    "    # retrait des nombres\n",
    "    temp = re.sub(\"[0-9]\", \"\", temp)\n",
    "    # tokénisation\n",
    "    temp = word_tokenize(temp)\n",
    "    # lemmatisation des termes\n",
    "    temp = [lem.lemmatize(mot) for mot in temp]\n",
    "    # retrait des stopwords\n",
    "    temp = [mot for mot in temp if not mot in stopwords]\n",
    "    # retirer les tokens de moins de 3 caractères\n",
    "    temp = [mot for mot in temp if len(mot) >= 3]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer le nettoyage au corpus\n",
    "corpus = list(data.tweet)\n",
    "corpus = [clean_tweet(doc, ponctuations, mots_vides, lem) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['school', 'grrr', 'mood', 'take', 'make', 'back', 'put', 'teacher', 'leave']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant:.........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>date_du_poste</th>\n",
       "      <th>utilisateur</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 20 00:27:23 PDT 2009</td>\n",
       "      <td>TapeLaLaLa</td>\n",
       "      <td>, school! GrRr im not in the mood to take my ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri May 29 13:29:39 PDT 2009</td>\n",
       "      <td>Billie_Anne</td>\n",
       "      <td>@pfchangs I can't direct message you for my fr...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat May 30 04:39:55 PDT 2009</td>\n",
       "      <td>rascal2pt0</td>\n",
       "      <td>Still not skinny. Up 0.4 lbs this morning</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 05 23:01:03 PDT 2009</td>\n",
       "      <td>Amaryste78</td>\n",
       "      <td>@PattinsonRobT Well some of us just want to sh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat Jun 06 11:30:11 PDT 2009</td>\n",
       "      <td>Alovepoet</td>\n",
       "      <td>extra credit for history is golden! scared abo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 31 18:31:36 PDT 2009</td>\n",
       "      <td>TataJones</td>\n",
       "      <td>@tommcfly back to Rio tomorrow,please. I need ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat Apr 18 07:15:28 PDT 2009</td>\n",
       "      <td>Ant_Cashanova</td>\n",
       "      <td>Slight hangover</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Wed Jun 17 12:16:33 PDT 2009</td>\n",
       "      <td>Bolanile</td>\n",
       "      <td>I must admit, I am jealous of the 88 degree we...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat Jun 20 04:32:00 PDT 2009</td>\n",
       "      <td>overhope</td>\n",
       "      <td>@nickvampie  sorry bro, after that evil one th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Tue Jun 16 18:27:54 PDT 2009</td>\n",
       "      <td>Amy_Pearson1993</td>\n",
       "      <td>@shaundiviney Shaun I have a problem :S wens I...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 date_du_poste      utilisateur  \\\n",
       "0      0  Mon Apr 20 00:27:23 PDT 2009       TapeLaLaLa   \n",
       "1      0  Fri May 29 13:29:39 PDT 2009      Billie_Anne   \n",
       "2      0  Sat May 30 04:39:55 PDT 2009       rascal2pt0   \n",
       "3      0  Fri Jun 05 23:01:03 PDT 2009       Amaryste78   \n",
       "4      0  Sat Jun 06 11:30:11 PDT 2009        Alovepoet   \n",
       "5      0  Sun May 31 18:31:36 PDT 2009        TataJones   \n",
       "6      0  Sat Apr 18 07:15:28 PDT 2009    Ant_Cashanova   \n",
       "7      0  Wed Jun 17 12:16:33 PDT 2009         Bolanile   \n",
       "8      0  Sat Jun 20 04:32:00 PDT 2009         overhope   \n",
       "9      0  Tue Jun 16 18:27:54 PDT 2009  Amy_Pearson1993   \n",
       "\n",
       "                                               tweet  polarite  \n",
       "0   , school! GrRr im not in the mood to take my ...  negative  \n",
       "1  @pfchangs I can't direct message you for my fr...  negative  \n",
       "2         Still not skinny. Up 0.4 lbs this morning   negative  \n",
       "3  @PattinsonRobT Well some of us just want to sh...  negative  \n",
       "4  extra credit for history is golden! scared abo...  negative  \n",
       "5  @tommcfly back to Rio tomorrow,please. I need ...  negative  \n",
       "6                                   Slight hangover   negative  \n",
       "7  I must admit, I am jealous of the 88 degree we...  negative  \n",
       "8  @nickvampie  sorry bro, after that evil one th...  negative  \n",
       "9  @shaundiviney Shaun I have a problem :S wens I...  negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nouvelle dataframe\n",
    "df_1 = pd.DataFrame({'label': data.polarite ,'tweet': corpus})\n",
    "print(\"avant:.........\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "après:.........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>[school, grrr, mood, take, make, back, put, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[cant, direct, message, free, lunch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>[still, skinny, morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>[well, want, share, sorry, hacked]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>[extra, credit, history, golden, scared, algre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>[back, rio, tomorrowplease, need, youi, need, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>[slight, hangover]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>[must, admit, jealous, degree, weather, today,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>negative</td>\n",
       "      <td>[sorry, bro, evil, one, ran, longer, back]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>negative</td>\n",
       "      <td>[shaun, problem, wen, preordered, album, put, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0  negative  [school, grrr, mood, take, make, back, put, te...\n",
       "1  negative               [cant, direct, message, free, lunch]\n",
       "2  negative                           [still, skinny, morning]\n",
       "3  negative                 [well, want, share, sorry, hacked]\n",
       "4  negative  [extra, credit, history, golden, scared, algre...\n",
       "5  negative  [back, rio, tomorrowplease, need, youi, need, ...\n",
       "6  negative                                 [slight, hangover]\n",
       "7  negative  [must, admit, jealous, degree, weather, today,...\n",
       "8  negative         [sorry, bro, evil, one, ran, longer, back]\n",
       "9  negative  [shaun, problem, wen, preordered, album, put, ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"après:.........\")\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# des tweets vides après nettoyage?\n",
    "print(df_1.loc[df_1.tweet==\"\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# retrait des tweets correspondants\n",
    "df_ok = df_1.loc[df_1.tweet != \"\"]\n",
    "print(df_ok.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>[school, grrr, mood, take, make, back, put, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[cant, direct, message, free, lunch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>[still, skinny, morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>[well, want, share, sorry, hacked]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>[extra, credit, history, golden, scared, algre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0  negative  [school, grrr, mood, take, make, back, put, te...\n",
       "1  negative               [cant, direct, message, free, lunch]\n",
       "2  negative                           [still, skinny, morning]\n",
       "3  negative                 [well, want, share, sorry, hacked]\n",
       "4  negative  [extra, credit, history, golden, scared, algre..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ok.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger une représentation pré-entraînée des termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../P7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2word2vec(glove_input_file=path+\"glove.twitter.27B.100d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A c ette instant j'ai utilisé glove2word2vec pour convertir des vecteurs GloVe au format texte au format texte word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193514, 100)\n"
     ]
    }
   ],
   "source": [
    "#dimension\n",
    "print(glove_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.0432e-01 -2.6150e-01 -1.6633e-01  4.6853e-01 -4.5815e-01  2.7206e-01\n",
      "  7.2837e-01 -2.7332e-01  7.1900e-03  1.3625e-02  1.2160e-01  5.8155e-01\n",
      " -1.9450e+00  7.3037e-02  3.2283e-01  7.7129e-01  4.1995e-01  3.6865e-01\n",
      "  1.3315e-01 -5.6905e-01  1.2460e-01 -4.2542e-01  2.1237e-01 -4.0740e-01\n",
      "  1.2356e+00  5.4340e-01 -2.2625e-01  3.0468e-02 -4.6312e-02  3.7316e-01\n",
      "  5.9658e-01  2.9139e-01  5.0718e-01 -2.9507e-01 -8.5241e-01  2.3822e-01\n",
      "  4.6839e-01 -1.8568e-01  1.7349e-01 -6.8062e-02  3.4812e-01 -1.8539e-01\n",
      " -1.3700e-01  2.5837e-01  7.0864e-01  8.3642e-01 -2.3989e-02  9.6164e-01\n",
      "  4.9107e-01  1.6798e-01  9.6609e-01 -2.8883e-01  3.8461e-01 -3.8119e-01\n",
      "  1.5834e-03 -1.1867e-01  2.7806e-01 -1.2263e-01  9.4348e-02  4.0990e-01\n",
      " -7.7709e-01 -2.3403e-01  1.5025e-01 -1.3776e-01 -7.2865e-01 -1.2810e-01\n",
      " -3.3601e-01  2.8861e-01 -2.6464e-01 -5.6897e-02  7.8214e-01 -4.2059e-01\n",
      " -2.2993e-01 -9.2306e-01 -7.5719e-02  8.3283e-02 -2.5813e-01  4.9935e-01\n",
      "  8.3127e-01 -3.5099e-01  1.7317e+00  6.3388e-01 -6.3036e-01  2.4671e-01\n",
      "  6.0489e-01 -2.4750e-01  1.4258e-01  7.5100e-02 -1.2801e-01  2.8027e-01\n",
      "  4.5831e-01 -1.2093e-01 -4.0274e-01  1.7500e-02  8.0021e-01  2.7186e-01\n",
      "  2.8896e-01 -5.8161e-01 -5.6983e-01  8.4957e-01]\n"
     ]
    }
   ],
   "source": [
    "#coordonnées de frog(grenouille)\n",
    "print(glove_model['frog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('squirrel', 0.7796489596366882), ('toad', 0.7313696146011353), ('turtle', 0.7177964448928833), ('rabbit', 0.7150670289993286), ('monkey', 0.6988509893417358), ('frogs', 0.6952397227287292), ('pig', 0.6924378871917725), ('unicorn', 0.6852490901947021), ('elephant', 0.6847099661827087), ('cow', 0.6808868646621704)]\n"
     ]
    }
   ],
   "source": [
    "#similarité avec frog(grenouille)\n",
    "print(glove_model.most_similar(['frog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7052315473556519), ('prince', 0.6666139364242554), ('mother', 0.6436765193939209), ('royal', 0.6417251229286194), ('father', 0.5952690243721008), ('african', 0.5883978009223938), ('princess', 0.5882176160812378), ('called', 0.5842776894569397), ('meets', 0.5840279459953308), ('american', 0.5815179944038391)]\n"
     ]
    }
   ],
   "source": [
    "#l'exemple emblématique \n",
    "print(glove_model.most_similar(positive=['king','woman'],negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordonnées des documents à partir des termes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation d'un document : moyenne des vecteurs des termes qui le composent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librairie numpy\n",
    "import numpy\n",
    "\n",
    "#fonction pour transformer un document en vecteur\n",
    "#à partir des tokens qui le composent\n",
    "#entrée : doc à traiter\n",
    "#         modèle préentrainé\n",
    "#sortie : vecteur représentant le document\n",
    "def my_doc_2_vec(doc,glove_model):\n",
    "    #dimension de représentation\n",
    "    p = glove_model.vectors.shape[1]\n",
    "    #initialiser le vecteur\n",
    "    vec = numpy.zeros(p)\n",
    "    #nombre de tokens trouvés\n",
    "    nb = 0\n",
    "    #traitement de chaque token du document\n",
    "    for tk in doc:\n",
    "        #ne traiter que les tokens reconnus\n",
    "        try:\n",
    "            values = glove_model[tk]\n",
    "            vec = vec + values\n",
    "            nb = nb + 1.0\n",
    "        except:\n",
    "            pass\n",
    "    #faire la moyenne des valeurs\n",
    "    #uniquement si on a trové des tokens reconnus bien sûr\n",
    "    if (nb > 0.0):\n",
    "        vec = vec/nb\n",
    "    #renvoyer le vecteur\n",
    "    #si aucun token trouvé, on a un vecteur de valeurs nulles\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100)\n"
     ]
    }
   ],
   "source": [
    "#traiter les documents du corpus corpus\n",
    "docsVec = list()\n",
    "#pour chaque document du corpus nettoyé\n",
    "for doc in df_ok['tweet']:\n",
    "    #calcul de son vecteur\n",
    "    vec = my_doc_2_vec(doc,glove_model)\n",
    "    #ajouter dans la liste\n",
    "    docsVec.append(vec)\n",
    "#transformer en matrice numpy\n",
    "matVec = numpy.array(docsVec)\n",
    "print(matVec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>v100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060899</td>\n",
       "      <td>0.198926</td>\n",
       "      <td>0.300655</td>\n",
       "      <td>0.129622</td>\n",
       "      <td>0.153767</td>\n",
       "      <td>0.075731</td>\n",
       "      <td>0.502758</td>\n",
       "      <td>0.147597</td>\n",
       "      <td>-0.013224</td>\n",
       "      <td>0.275639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121738</td>\n",
       "      <td>0.097694</td>\n",
       "      <td>0.068696</td>\n",
       "      <td>0.199511</td>\n",
       "      <td>-0.059477</td>\n",
       "      <td>-0.123621</td>\n",
       "      <td>-0.122238</td>\n",
       "      <td>0.067667</td>\n",
       "      <td>-0.064020</td>\n",
       "      <td>0.135521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081094</td>\n",
       "      <td>0.560460</td>\n",
       "      <td>-0.498200</td>\n",
       "      <td>0.070908</td>\n",
       "      <td>-0.185006</td>\n",
       "      <td>-0.251941</td>\n",
       "      <td>0.469116</td>\n",
       "      <td>0.095297</td>\n",
       "      <td>0.361210</td>\n",
       "      <td>0.265204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.239547</td>\n",
       "      <td>0.203879</td>\n",
       "      <td>-0.086067</td>\n",
       "      <td>0.040713</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>0.304194</td>\n",
       "      <td>-0.134565</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>-0.293732</td>\n",
       "      <td>-0.070785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065943</td>\n",
       "      <td>-0.301713</td>\n",
       "      <td>0.122880</td>\n",
       "      <td>0.082153</td>\n",
       "      <td>-0.053890</td>\n",
       "      <td>0.291073</td>\n",
       "      <td>0.279334</td>\n",
       "      <td>0.174660</td>\n",
       "      <td>-0.118000</td>\n",
       "      <td>0.184008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023398</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>0.312067</td>\n",
       "      <td>0.094859</td>\n",
       "      <td>-0.150200</td>\n",
       "      <td>-0.014259</td>\n",
       "      <td>-0.206707</td>\n",
       "      <td>0.371103</td>\n",
       "      <td>0.253408</td>\n",
       "      <td>-0.180955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.281499</td>\n",
       "      <td>0.637894</td>\n",
       "      <td>-0.124118</td>\n",
       "      <td>0.160039</td>\n",
       "      <td>-0.456024</td>\n",
       "      <td>-0.284932</td>\n",
       "      <td>0.430783</td>\n",
       "      <td>-0.068897</td>\n",
       "      <td>0.594408</td>\n",
       "      <td>0.392441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172953</td>\n",
       "      <td>0.034142</td>\n",
       "      <td>-0.032593</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.047503</td>\n",
       "      <td>-0.360497</td>\n",
       "      <td>-0.282181</td>\n",
       "      <td>0.101678</td>\n",
       "      <td>0.278702</td>\n",
       "      <td>0.155895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.218933</td>\n",
       "      <td>0.314345</td>\n",
       "      <td>0.312355</td>\n",
       "      <td>0.019018</td>\n",
       "      <td>-0.149179</td>\n",
       "      <td>-0.114710</td>\n",
       "      <td>0.281285</td>\n",
       "      <td>0.022573</td>\n",
       "      <td>0.071846</td>\n",
       "      <td>0.219789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056736</td>\n",
       "      <td>-0.128943</td>\n",
       "      <td>-0.070469</td>\n",
       "      <td>-0.230003</td>\n",
       "      <td>0.256608</td>\n",
       "      <td>0.283126</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.313177</td>\n",
       "      <td>-0.004764</td>\n",
       "      <td>0.151090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0  0.060899  0.198926  0.300655  0.129622  0.153767  0.075731  0.502758   \n",
       "1  0.081094  0.560460 -0.498200  0.070908 -0.185006 -0.251941  0.469116   \n",
       "2 -0.065943 -0.301713  0.122880  0.082153 -0.053890  0.291073  0.279334   \n",
       "3  0.281499  0.637894 -0.124118  0.160039 -0.456024 -0.284932  0.430783   \n",
       "4 -0.218933  0.314345  0.312355  0.019018 -0.149179 -0.114710  0.281285   \n",
       "\n",
       "         v8        v9       v10  ...       v91       v92       v93       v94  \\\n",
       "0  0.147597 -0.013224  0.275639  ... -0.121738  0.097694  0.068696  0.199511   \n",
       "1  0.095297  0.361210  0.265204  ... -0.239547  0.203879 -0.086067  0.040713   \n",
       "2  0.174660 -0.118000  0.184008  ... -0.023398  0.132623  0.312067  0.094859   \n",
       "3 -0.068897  0.594408  0.392441  ... -0.172953  0.034142 -0.032593  0.004753   \n",
       "4  0.022573  0.071846  0.219789  ... -0.056736 -0.128943 -0.070469 -0.230003   \n",
       "\n",
       "        v95       v96       v97       v98       v99      v100  \n",
       "0 -0.059477 -0.123621 -0.122238  0.067667 -0.064020  0.135521  \n",
       "1  0.193605  0.304194 -0.134565  0.035534 -0.293732 -0.070785  \n",
       "2 -0.150200 -0.014259 -0.206707  0.371103  0.253408 -0.180955  \n",
       "3  0.047503 -0.360497 -0.282181  0.101678  0.278702  0.155895  \n",
       "4  0.256608  0.283126  0.009690  0.313177 -0.004764  0.151090  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transformer en data frame\n",
    "df = pd.DataFrame(matVec,columns=[\"v\"+str(i+1) for i in range(matVec.shape[1])])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>v100</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060899</td>\n",
       "      <td>0.198926</td>\n",
       "      <td>0.300655</td>\n",
       "      <td>0.129622</td>\n",
       "      <td>0.153767</td>\n",
       "      <td>0.075731</td>\n",
       "      <td>0.502758</td>\n",
       "      <td>0.147597</td>\n",
       "      <td>-0.013224</td>\n",
       "      <td>0.275639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097694</td>\n",
       "      <td>0.068696</td>\n",
       "      <td>0.199511</td>\n",
       "      <td>-0.059477</td>\n",
       "      <td>-0.123621</td>\n",
       "      <td>-0.122238</td>\n",
       "      <td>0.067667</td>\n",
       "      <td>-0.064020</td>\n",
       "      <td>0.135521</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081094</td>\n",
       "      <td>0.560460</td>\n",
       "      <td>-0.498200</td>\n",
       "      <td>0.070908</td>\n",
       "      <td>-0.185006</td>\n",
       "      <td>-0.251941</td>\n",
       "      <td>0.469116</td>\n",
       "      <td>0.095297</td>\n",
       "      <td>0.361210</td>\n",
       "      <td>0.265204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203879</td>\n",
       "      <td>-0.086067</td>\n",
       "      <td>0.040713</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>0.304194</td>\n",
       "      <td>-0.134565</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>-0.293732</td>\n",
       "      <td>-0.070785</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.065943</td>\n",
       "      <td>-0.301713</td>\n",
       "      <td>0.122880</td>\n",
       "      <td>0.082153</td>\n",
       "      <td>-0.053890</td>\n",
       "      <td>0.291073</td>\n",
       "      <td>0.279334</td>\n",
       "      <td>0.174660</td>\n",
       "      <td>-0.118000</td>\n",
       "      <td>0.184008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>0.312067</td>\n",
       "      <td>0.094859</td>\n",
       "      <td>-0.150200</td>\n",
       "      <td>-0.014259</td>\n",
       "      <td>-0.206707</td>\n",
       "      <td>0.371103</td>\n",
       "      <td>0.253408</td>\n",
       "      <td>-0.180955</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.281499</td>\n",
       "      <td>0.637894</td>\n",
       "      <td>-0.124118</td>\n",
       "      <td>0.160039</td>\n",
       "      <td>-0.456024</td>\n",
       "      <td>-0.284932</td>\n",
       "      <td>0.430783</td>\n",
       "      <td>-0.068897</td>\n",
       "      <td>0.594408</td>\n",
       "      <td>0.392441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034142</td>\n",
       "      <td>-0.032593</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.047503</td>\n",
       "      <td>-0.360497</td>\n",
       "      <td>-0.282181</td>\n",
       "      <td>0.101678</td>\n",
       "      <td>0.278702</td>\n",
       "      <td>0.155895</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.218933</td>\n",
       "      <td>0.314345</td>\n",
       "      <td>0.312355</td>\n",
       "      <td>0.019018</td>\n",
       "      <td>-0.149179</td>\n",
       "      <td>-0.114710</td>\n",
       "      <td>0.281285</td>\n",
       "      <td>0.022573</td>\n",
       "      <td>0.071846</td>\n",
       "      <td>0.219789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128943</td>\n",
       "      <td>-0.070469</td>\n",
       "      <td>-0.230003</td>\n",
       "      <td>0.256608</td>\n",
       "      <td>0.283126</td>\n",
       "      <td>0.009690</td>\n",
       "      <td>0.313177</td>\n",
       "      <td>-0.004764</td>\n",
       "      <td>0.151090</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0  0.060899  0.198926  0.300655  0.129622  0.153767  0.075731  0.502758   \n",
       "1  0.081094  0.560460 -0.498200  0.070908 -0.185006 -0.251941  0.469116   \n",
       "2 -0.065943 -0.301713  0.122880  0.082153 -0.053890  0.291073  0.279334   \n",
       "3  0.281499  0.637894 -0.124118  0.160039 -0.456024 -0.284932  0.430783   \n",
       "4 -0.218933  0.314345  0.312355  0.019018 -0.149179 -0.114710  0.281285   \n",
       "\n",
       "         v8        v9       v10  ...       v92       v93       v94       v95  \\\n",
       "0  0.147597 -0.013224  0.275639  ...  0.097694  0.068696  0.199511 -0.059477   \n",
       "1  0.095297  0.361210  0.265204  ...  0.203879 -0.086067  0.040713  0.193605   \n",
       "2  0.174660 -0.118000  0.184008  ...  0.132623  0.312067  0.094859 -0.150200   \n",
       "3 -0.068897  0.594408  0.392441  ...  0.034142 -0.032593  0.004753  0.047503   \n",
       "4  0.022573  0.071846  0.219789  ... -0.128943 -0.070469 -0.230003  0.256608   \n",
       "\n",
       "        v96       v97       v98       v99      v100     label  \n",
       "0 -0.123621 -0.122238  0.067667 -0.064020  0.135521  negative  \n",
       "1  0.304194 -0.134565  0.035534 -0.293732 -0.070785  negative  \n",
       "2 -0.014259 -0.206707  0.371103  0.253408 -0.180955  negative  \n",
       "3 -0.360497 -0.282181  0.101678  0.278702  0.155895  negative  \n",
       "4  0.283126  0.009690  0.313177 -0.004764  0.151090  negative  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajouter la classe\n",
    "df['label'] = df_ok.label\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction et évaluation en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 101)\n",
      "(12000, 101)\n"
     ]
    }
   ],
   "source": [
    "#partition apprentissage test\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTrain, dfTest = train_test_split(df,train_size=0.7,stratify=df.label,random_state=0)\n",
    "print(dfTrain.shape)\n",
    "print(dfTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je mets un échantillonage stratifié pour avoir les mêmes proportion de classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM avec un noyau RBF par défaut\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(dfTrain[dfTrain.columns[:-1]],dfTrain.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "#prédiction en test\n",
    "pred = clf.predict(dfTest[dfTest.columns[:-1]])\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.74      0.74      6000\n",
      "    positive       0.74      0.75      0.74      6000\n",
      "\n",
      "    accuracy                           0.74     12000\n",
      "   macro avg       0.74      0.74      0.74     12000\n",
      "weighted avg       0.74      0.74      0.74     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#évaluation des performances\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(dfTest.label,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déploiement sur un document supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document à traiter\n",
    "blog_avion = \"My next plane ticket has just been booked! I fly in March to the Philippines. I am currently in the process of preparing for my next trip: what are the places I would like to discover, which itinerary to choose and in which cities to sleep, which domestic flights I will book to move between the islands etc. A trip by plane is an action that is anticipated and prepared a minimum. We can also face hazards, and it is important to know how to react. I indicate in this article various and varied tips on the theme of air travel. During the flight, the time may seem long, especially depending on the duration of your trip. Take something to take care of, and what not to be embarrassed (Quies balls can be useful). Also, feel free to get up from time to time if you need to stretch your legs. Be aware that the toilet is unavailable during take-off and landing, take precautions. A tip, always carry a photocopy of your passport, it can help in case of loss or theft that would occur during the trip. We also often talk about flight delay or cancellation. The wait seems endless in these cases, and travelers too often lack information. It is therefore necessary to try to take his evil in patience. Delayed plane, cancelled flight,companies like Indemnflight can help you get compensated. Note that the delay must be more than 3 hours for compensation to be possible. The advantage of going through Indemnflight is that experts take care of the administrative procedures for you, and they know the rights of passengers well. Note that they are paid only when the compensation is validated. You therefore delegate the management of the request, and then pay them in return a part of the compensation. I personally have only 2h30 maximum of delay to my credit, and have never tested the claim. Finally, the loss of luggage is also a thorny subject! If you do not find it at the exit of the plane, go to a counter available at the airport to report it and find a solution. The price of a plane ticket varies a lot depending on many factors. The level of demand, the time of year, the day and time of booking and the booking time before the departure date are very important elements to take into account if you have in mind to make a good deal when booking your flight. To learn more, I advise you to read this article from my travel blog: How to find a cheap plane ticket? Follow these 11 tips! When booking your flight, check that the formalities of entry into the desired country are feasible on time (example: a visa application must be made in advance), and that your identity card and/or passport will still be valid. Note that for many destinations, the passport must often be valid for a period of 6 months from the date of arrival. It is advisable to book flights well in advance. You rarely get a good deal when you buy a plane ticket a few days before departure. And in this case we take the risk of no longer having a place. In the case of my next trip to the Philippines, it is for these reasons that I will soon book the 2 domestic flights. Especially since I am going to travel with the national airline, and I have the impression that the weight of the luggage allowed is less than the 23 kg allowed on my international flight. I must therefore take this into account. Finally, it is necessary to pay attention to the information provided during the reservation. An error on the last name, or a time on the dates can be expensive, there is in this case a risk of paying extra to make this type of change, or even having to cancel your trip. First of all, find out a little in advance about how you will reach the airport (car + parking, bus, train...). When packing your suitcase, try to think of everything you will need on the spot. Also be aware that some products are prohibited in cabin baggage (liquid products greater than 100ml, knives etc.). Check the airline's website for an exhaustive list, and avoid having certain products confiscated. It is advisable to arrive 1h30 to 2h in advance for short or medium haul flight. 2h to 3h are recommended for international flights. This delay varies according to several elements: is your plane ticket ready or are you going to do the formalities at the airport? Do you have luggage to drop off at the counter? How big is the airport and in which area will you board? To help you and go further, discover the guide: practical tips for flying the first time: the steps to follow at the airport. Do you have any other tips to share when flying? Or adventures to share? Feel free to indicate them in the comments.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['next', 'plane', 'ticket', 'booked', 'fly', 'march', 'philippine', 'currently', 'process', 'preparing', 'next', 'trip', 'place', 'would', 'like', 'discover', 'itinerary', 'choose', 'city', 'sleep', 'domestic', 'flight', 'book', 'move', 'island', 'etc', 'trip', 'plane', 'action', 'anticipated', 'prepared', 'minimum', 'also', 'face', 'hazard', 'important', 'know', 'react', 'indicate', 'article', 'various', 'varied', 'tip', 'theme', 'air', 'travel', 'flight', 'time', 'may', 'seem', 'long', 'especially', 'depending', 'duration', 'trip', 'take', 'something', 'take', 'care', 'embarrassed', 'quies', 'ball', 'useful', 'also', 'feel', 'free', 'get', 'time', 'time', 'need', 'stretch', 'leg', 'aware', 'toilet', 'unavailable', 'takeoff', 'landing', 'take', 'precaution', 'tip', 'always', 'carry', 'photocopy', 'passport', 'help', 'case', 'loss', 'theft', 'would', 'occur', 'trip', 'also', 'often', 'talk', 'flight', 'delay', 'cancellation', 'wait', 'seems', 'endless', 'case', 'traveler', 'often', 'lack', 'information', 'therefore', 'necessary', 'try', 'take', 'evil', 'patience', 'delayed', 'plane', 'cancelled', 'flightcompanies', 'like', 'indemnflight', 'help', 'get', 'compensated', 'note', 'delay', 'must', 'hour', 'compensation', 'possible', 'advantage', 'going', 'indemnflight', 'expert', 'take', 'care', 'administrative', 'procedure', 'know', 'right', 'passenger', 'well', 'note', 'paid', 'compensation', 'validated', 'therefore', 'delegate', 'management', 'request', 'pay', 'return', 'part', 'compensation', 'personally', 'maximum', 'delay', 'credit', 'never', 'tested', 'claim', 'finally', 'loss', 'luggage', 'also', 'thorny', 'subject', 'find', 'exit', 'plane', 'counter', 'available', 'airport', 'report', 'find', 'solution', 'price', 'plane', 'ticket', 'varies', 'lot', 'depending', 'many', 'factor', 'level', 'demand', 'time', 'year', 'day', 'time', 'booking', 'booking', 'time', 'departure', 'date', 'important', 'element', 'take', 'account', 'mind', 'make', 'good', 'deal', 'booking', 'flight', 'learn', 'advise', 'read', 'article', 'travel', 'blog', 'find', 'cheap', 'plane', 'ticket', 'follow', 'tip', 'booking', 'flight', 'check', 'formality', 'entry', 'desired', 'country', 'feasible', 'time', 'example', 'visa', 'application', 'must', 'made', 'advance', 'identity', 'card', 'andor', 'passport', 'still', 'valid', 'note', 'many', 'destination', 'passport', 'must', 'often', 'valid', 'period', 'month', 'date', 'arrival', 'advisable', 'book', 'flight', 'well', 'advance', 'rarely', 'get', 'good', 'deal', 'buy', 'plane', 'ticket', 'day', 'departure', 'case', 'take', 'risk', 'longer', 'place', 'case', 'next', 'trip', 'philippine', 'reason', 'soon', 'book', 'domestic', 'flight', 'especially', 'since', 'going', 'travel', 'national', 'airline', 'impression', 'weight', 'luggage', 'allowed', 'allowed', 'international', 'flight', 'must', 'therefore', 'take', 'account', 'finally', 'necessary', 'pay', 'attention', 'information', 'provided', 'reservation', 'error', 'last', 'name', 'time', 'date', 'expensive', 'case', 'risk', 'paying', 'extra', 'make', 'type', 'change', 'even', 'cancel', 'trip', 'first', 'find', 'little', 'advance', 'reach', 'airport', 'car', 'parking', 'bus', 'train', 'packing', 'suitcase', 'try', 'think', 'everything', 'need', 'spot', 'also', 'aware', 'product', 'prohibited', 'cabin', 'baggage', 'liquid', 'product', 'greater', 'knife', 'etc', 'check', 'airline', 'website', 'exhaustive', 'list', 'avoid', 'certain', 'product', 'confiscated', 'advisable', 'arrive', 'advance', 'short', 'medium', 'haul', 'flight', 'recommended', 'international', 'flight', 'delay', 'varies', 'according', 'several', 'element', 'plane', 'ticket', 'ready', 'going', 'formality', 'airport', 'luggage', 'drop', 'counter', 'big', 'airport', 'area', 'board', 'help', 'discover', 'guide', 'practical', 'tip', 'flying', 'first', 'time', 'step', 'follow', 'airport', 'tip', 'share', 'flying', 'adventure', 'share', 'feel', 'free', 'indicate', 'comment']\n"
     ]
    }
   ],
   "source": [
    "# nettoyage\n",
    "my_clean = clean_tweet(blog_avion, ponctuations, mots_vides,lem)\n",
    "print(my_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation du document en vecteur et prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.42997222e-02  1.16145684e-01  3.87272052e-02 -1.20921594e-01\n",
      "  1.08192906e-01  4.18694296e-02  3.35730307e-01 -1.17281475e-01\n",
      "  6.41943145e-02  3.38872367e-02  2.64979805e-02 -1.73002679e-01\n",
      " -3.18489433e+00  9.33958689e-02  2.81091918e-02 -6.14780135e-02\n",
      "  3.22580343e-02  1.22408663e-01 -8.05992555e-02 -2.37422325e-01\n",
      " -8.49119562e-02 -1.36923613e-01 -1.93114940e-02  3.23095252e-02\n",
      " -1.43564945e-01  2.80699916e-01  1.00997381e-01  9.97032886e-02\n",
      "  4.53340388e-02  7.60205715e-02  4.45375788e-02  5.21420077e-02\n",
      " -2.46802528e-01 -2.34085313e-02  2.36621924e-01  4.01339422e-02\n",
      "  1.23273508e-01  6.53882959e-02  1.12388811e-01  2.54380495e-02\n",
      " -2.53524606e-01  1.24888294e-01  1.72107432e-01 -8.64640767e-02\n",
      "  3.44709201e-01 -3.41666089e-02  8.14689056e-02 -1.96896431e-01\n",
      " -6.96362389e-02  3.10120713e-01  1.52974121e-01  8.52233496e-02\n",
      " -2.34912676e-01  1.09456171e-01 -1.24663969e-01  1.82868192e-01\n",
      "  1.75005275e-02 -6.22283272e-02 -8.58723371e-02 -1.27122005e-02\n",
      "  2.45708276e-01  1.89394542e-01  1.53912921e-01  1.37460393e-01\n",
      "  1.72327562e-01  5.02909732e-03  3.99698829e-02 -2.37629462e-01\n",
      "  1.45770880e-03 -1.10905823e-03 -1.59962529e-01  9.06528723e-02\n",
      "  8.49871031e-02 -3.09343627e-02  2.94879235e-01 -1.81319721e-01\n",
      " -1.22104064e-01 -1.28390295e-01  1.01129110e-01 -1.78165008e-01\n",
      "  1.17053590e+00  2.79349011e-02  1.31184582e-01  8.25558649e-04\n",
      "  2.01891766e-01  3.13332157e-05 -3.14771842e-02 -1.25767190e-01\n",
      "  8.04232635e-02 -8.17614336e-03 -1.67886872e-01  1.84558484e-01\n",
      " -2.07563103e-02 -7.00142071e-02 -4.83331830e-02 -1.00677514e-01\n",
      " -9.66500618e-02  1.60146056e-02 -8.94700610e-02 -6.01191370e-02]\n"
     ]
    }
   ],
   "source": [
    "#avec le modèle pré-entrainé\n",
    "#et avec la fonction ci-dessus\n",
    "my_vec = my_doc_2_vec(my_clean,glove_model)\n",
    "print(my_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "#prédiction avec le SVM\n",
    "pred_my_doc = clf.predict(my_vec.reshape(1,-1))\n",
    "\n",
    "#le commentaire est de nature positive\n",
    "print(pred_my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarder le modèle Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P7_01_modèlesurmesure_04_embedding_Glove.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# maintenant je l’enregistre dans un fichier\n",
    "joblib.dump(clf, 'P7_01_modèlesurmesure_04_embedding_Glove.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

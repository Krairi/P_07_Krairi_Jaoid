{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7 : Détectez les Bad Buzz grâce au Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement du contenu des tweets (pour modèle word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>date_du_poste</th>\n",
       "      <th>utilisateur</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 date_du_poste    utilisateur  \\\n",
       "0      0  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
       "1      0  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
       "2      0  Mon Apr 06 22:19:57 PDT 2009        ElleCTF   \n",
       "3      0  Mon Apr 06 22:19:57 PDT 2009         Karoli   \n",
       "4      0  Mon Apr 06 22:20:00 PDT 2009       joy_wolf   \n",
       "\n",
       "                                               tweet  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "df_dep = pd.read_csv('P7_03_fichiercsv_tweet.csv')\n",
    "df_dep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    799999\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dep.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep['polarite'] = df_dep['label'].map({\n",
    "    4 : 'positive',\n",
    "    0 : 'negative'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    800000\n",
       "negative    799999\n",
       "Name: polarite, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dep.polarite.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrait des données\n",
    "n_sample = 20000\n",
    "df_neg= df_dep[df_dep['polarite'] == 'negative'].sample(n=n_sample)\n",
    "df_pos = df_dep[df_dep['polarite'] == 'positive'].sample(n=n_sample)\n",
    "\n",
    "data = pd.concat([df_neg, df_pos], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    20000\n",
       "positive    20000\n",
       "Name: polarite, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.polarite.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
      "\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JK253\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JK253\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JK253\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#*****************************************\n",
    "# Préparation des librairie et des outils\n",
    "# pour le nettoyage des tweets\n",
    "#*****************************************\n",
    "\n",
    "# Expressions régulières\n",
    "import re\n",
    "\n",
    "# Pontuations\n",
    "import string\n",
    "ponctuations = list(string.punctuation)\n",
    "print(ponctuations)\n",
    "\n",
    "# Tokénisation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Lemmatisation\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Charger les stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "mots_vides = stopwords.words('english')\n",
    "print('\\n')\n",
    "print(mots_vides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer le contenu des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour nettoyage de chaque document\n",
    "# tweet = corps du tweet = document\n",
    "# ponctuations : liste des ponctuations\n",
    "# stopwords : liste des stopwords à retirer\n",
    "# lem : fonction pour la lemmatisation des termes\n",
    "\n",
    "def clean_tweet(tweet, ponctuations, stopwords, lem):\n",
    "    # Harmonisation de la casse\n",
    "    temp = tweet.lower()\n",
    "    # retier les contractions en anglais\n",
    "    temp = re.sub(\"'\", \"\", temp)\n",
    "    # retrait des @\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\", \"\", temp)\n",
    "    # retrait des #\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\", \"\", temp)\n",
    "    # retrait des liens web (http et https)\n",
    "    temp = re.sub(r'http\\S+','', temp)\n",
    "    # retrait des ponctuations\n",
    "    temp = \"\".join([char for char in list(temp) if not (char in ponctuations)])\n",
    "    # retrait des nombres\n",
    "    temp = re.sub(\"[0-9]\", \"\", temp)\n",
    "    # tokénisation\n",
    "    temp = word_tokenize(temp)\n",
    "    # lemmatisation des termes\n",
    "    temp = [lem.lemmatize(mot) for mot in temp]\n",
    "    # retrait des stopwords\n",
    "    temp = [mot for mot in temp if not mot in stopwords]\n",
    "    # retirer les tokens de moins de 3 caractères\n",
    "    temp = [mot for mot in temp if len(mot) >= 3]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer le nettoyage au corpus\n",
    "corpus = list(data.tweet)\n",
    "corpus = [clean_tweet(doc, ponctuations, mots_vides, lem) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nooo',\n",
       " 'theyre',\n",
       " 'together',\n",
       " 'kstew',\n",
       " 'boyf',\n",
       " 'like',\n",
       " 'year',\n",
       " 'sth',\n",
       " 'man',\n",
       " 'like',\n",
       " 'see',\n",
       " 'together',\n",
       " 'though']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant:.........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>date_du_poste</th>\n",
       "      <th>utilisateur</th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat Jun 20 23:20:52 PDT 2009</td>\n",
       "      <td>hmtangx</td>\n",
       "      <td>@ali_mwahxx nooo they're not together! kstew h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 31 10:49:22 PDT 2009</td>\n",
       "      <td>semipenguin</td>\n",
       "      <td>http://twitpic.com/6cflk - People would rather...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Fri Jun 19 04:19:37 PDT 2009</td>\n",
       "      <td>merlinc</td>\n",
       "      <td>Yurgh. My code smells really badly today. Defi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Wed Jun 17 08:53:53 PDT 2009</td>\n",
       "      <td>sleepingqueen</td>\n",
       "      <td>@minnaeii thats very true na! people cant come...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jun 22 18:58:21 PDT 2009</td>\n",
       "      <td>Vaes_mama</td>\n",
       "      <td>Has a bad headache...hasn't felt right in two ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Thu Jun 18 22:04:29 PDT 2009</td>\n",
       "      <td>tRAS</td>\n",
       "      <td>Day turning out to be disastrous. Lost bid in ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun May 31 19:12:14 PDT 2009</td>\n",
       "      <td>MrsHardy</td>\n",
       "      <td>Where is the preview</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 18 04:25:18 PDT 2009</td>\n",
       "      <td>BabyVanessa093</td>\n",
       "      <td>I'm siiick  Well at least no school today xD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Sat Apr 18 08:56:38 PDT 2009</td>\n",
       "      <td>JasonpPDX</td>\n",
       "      <td>Saturday Class is no good</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jun 15 11:38:30 PDT 2009</td>\n",
       "      <td>originalsteven</td>\n",
       "      <td>Back in Paisley</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                 date_du_poste     utilisateur  \\\n",
       "0      0  Sat Jun 20 23:20:52 PDT 2009         hmtangx   \n",
       "1      0  Sun May 31 10:49:22 PDT 2009     semipenguin   \n",
       "2      0  Fri Jun 19 04:19:37 PDT 2009         merlinc   \n",
       "3      0  Wed Jun 17 08:53:53 PDT 2009   sleepingqueen   \n",
       "4      0  Mon Jun 22 18:58:21 PDT 2009       Vaes_mama   \n",
       "5      0  Thu Jun 18 22:04:29 PDT 2009            tRAS   \n",
       "6      0  Sun May 31 19:12:14 PDT 2009        MrsHardy   \n",
       "7      0  Mon May 18 04:25:18 PDT 2009  BabyVanessa093   \n",
       "8      0  Sat Apr 18 08:56:38 PDT 2009       JasonpPDX   \n",
       "9      0  Mon Jun 15 11:38:30 PDT 2009  originalsteven   \n",
       "\n",
       "                                               tweet  polarite  \n",
       "0  @ali_mwahxx nooo they're not together! kstew h...  negative  \n",
       "1  http://twitpic.com/6cflk - People would rather...  negative  \n",
       "2  Yurgh. My code smells really badly today. Defi...  negative  \n",
       "3  @minnaeii thats very true na! people cant come...  negative  \n",
       "4  Has a bad headache...hasn't felt right in two ...  negative  \n",
       "5  Day turning out to be disastrous. Lost bid in ...  negative  \n",
       "6                              Where is the preview   negative  \n",
       "7       I'm siiick  Well at least no school today xD  negative  \n",
       "8                         Saturday Class is no good   negative  \n",
       "9                                   Back in Paisley   negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nouvelle dataframe\n",
    "df_1 = pd.DataFrame({'label': data.polarite ,'tweet': corpus})\n",
    "print(\"avant:.........\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "après:.........\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>[nooo, theyre, together, kstew, boyf, like, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>[people, would, rather, watch, death, wish, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>[yurgh, code, smell, really, badly, today, def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>[thats, true, people, cant, come, peace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>[bad, headachehasnt, felt, right, two, day, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>[day, turning, disastrous, lost, bid, ebay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>[preview]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>negative</td>\n",
       "      <td>[siiick, well, least, school, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>negative</td>\n",
       "      <td>[saturday, class, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>negative</td>\n",
       "      <td>[back, paisley]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0  negative  [nooo, theyre, together, kstew, boyf, like, ye...\n",
       "1  negative  [people, would, rather, watch, death, wish, na...\n",
       "2  negative  [yurgh, code, smell, really, badly, today, def...\n",
       "3  negative           [thats, true, people, cant, come, peace]\n",
       "4  negative  [bad, headachehasnt, felt, right, two, day, fi...\n",
       "5  negative        [day, turning, disastrous, lost, bid, ebay]\n",
       "6  negative                                          [preview]\n",
       "7  negative               [siiick, well, least, school, today]\n",
       "8  negative                            [saturday, class, good]\n",
       "9  negative                                    [back, paisley]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"après:.........\")\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# des tweets vides après nettoyage?\n",
    "print(df_1.loc[df_1.tweet==\"\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# retrait des tweets correspondants\n",
    "df_ok = df_1.loc[df_1.tweet != \"\"]\n",
    "print(df_ok.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELE WOR2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger une représentation pré-entraînée des termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../P7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chargement\n",
    "import gensim\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "trained = word2vec.KeyedVectors.load_word2vec_format(path+\"enwiki_20180420_100d.txt.bz2\",binary=False,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4530030, 100)\n"
     ]
    }
   ],
   "source": [
    "#dimension\n",
    "print(trained.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5573  0.4365  0.1881  0.3147  0.6119  0.2175  0.6588  0.2651  0.4745\n",
      " -0.5842  0.368   0.7477  0.3323 -1.0548 -0.1766 -0.2134 -0.0887  0.1165\n",
      "  0.4452  0.7476  0.1865  0.1122 -0.6044 -0.6743 -1.1116 -0.4325  0.0572\n",
      " -0.2212  0.2282  1.3615  1.151  -0.1707  0.0887 -0.0732  0.4425 -0.2659\n",
      " -0.4304  0.6612  0.4771  0.12    0.4184  0.6925 -0.5642  0.1899  0.3655\n",
      "  0.4986 -0.2736 -0.2063 -0.332   0.1967  0.8136  0.1608 -0.337  -0.5333\n",
      "  0.2754 -0.0651 -0.2192  1.0819  0.3567 -0.1346  0.1839 -0.7201 -0.1903\n",
      "  0.3891  1.6501 -0.0998 -0.1689 -0.4305 -0.5942  0.0181  0.2077 -0.7044\n",
      " -0.2112 -0.3843 -1.4317  0.002   0.0768 -0.5053 -0.6565 -0.3063  0.1394\n",
      " -0.1958 -0.0881 -0.6662 -0.0591  0.1755  0.3093  0.2115  0.3597 -0.2962\n",
      "  0.4297 -0.0404  0.3238  0.4451  0.8552 -0.4519 -0.3047  0.0202  0.2848\n",
      " -0.1223]\n"
     ]
    }
   ],
   "source": [
    "#coordonnées de mcenroe(joueur de tenis)\n",
    "print(trained['mcenroe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ENTITY/John_McEnroe', 0.8751851916313171), ('gerulaitis', 0.8309396505355835), ('wilander', 0.8211947679519653), ('ENTITY/Wojciech_Fibak', 0.8087114691734314), ('kafelnikov', 0.8011326193809509), ('ENTITY/Stefan_Edberg', 0.8009007573127747), ('lendl', 0.8008507490158081), ('sampras', 0.7997835874557495), ('ENTITY/Peter_Fleming_(tennis)', 0.7964615225791931), ('ENTITY/Jim_Grabb', 0.7897493839263916)]\n"
     ]
    }
   ],
   "source": [
    "#similarité avec mcenroe(joueur de tennis)\n",
    "print(trained.most_similar(['mcenroe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.8306491374969482), ('monarch', 0.7416261434555054), ('ENTITY/Queen_consort', 0.7348717451095581), ('laungshe', 0.7347309589385986), ('regnant', 0.7243735194206238), ('chelna', 0.7236213684082031), ('consort', 0.720160722732544), ('indlovukati', 0.7181541919708252), ('kamamalu', 0.7178552150726318), ('indlovukazi', 0.714848518371582)]\n"
     ]
    }
   ],
   "source": [
    "#l'exemple emblématique de word2vec\n",
    "print(trained.most_similar(positive=['king','woman'],negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordonnées des documents à partir des termes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation d'un document : moyenne des vecteurs des termes qui le composent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librairie numpy\n",
    "import numpy\n",
    "\n",
    "#fonction pour transformer un document en vecteur\n",
    "#à partir des tokens qui le composent\n",
    "#entrée : doc à traiter\n",
    "#         modèle préentrainé\n",
    "#sortie : vecteur représentant le document\n",
    "def my_doc_2_vec(doc,trained):\n",
    "    #dimension de représentation\n",
    "    p = trained.vectors.shape[1]\n",
    "    #initialiser le vecteur\n",
    "    vec = numpy.zeros(p)\n",
    "    #nombre de tokens trouvés\n",
    "    nb = 0\n",
    "    #traitement de chaque token du document\n",
    "    for tk in doc:\n",
    "        #ne traiter que les tokens reconnus\n",
    "        try:\n",
    "            values = trained[tk]\n",
    "            vec = vec + values\n",
    "            nb = nb + 1.0\n",
    "        except:\n",
    "            pass\n",
    "    #faire la moyenne des valeurs\n",
    "    #uniquement si on a trové des tokens reconnus bien sûr\n",
    "    if (nb > 0.0):\n",
    "        vec = vec/nb\n",
    "    #renvoyer le vecteur\n",
    "    #si aucun token trouvé, on a un vecteur de valeurs nulles\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 100)\n"
     ]
    }
   ],
   "source": [
    "#traiter les documents du corpus corpus\n",
    "docsVec = list()\n",
    "#pour chaque document du corpus nettoyé\n",
    "for doc in df_ok['tweet']:\n",
    "    #calcul de son vecteur\n",
    "    vec = my_doc_2_vec(doc,trained)\n",
    "    #ajouter dans la liste\n",
    "    docsVec.append(vec)\n",
    "#transformer en matrice numpy\n",
    "matVec = numpy.array(docsVec)\n",
    "print(matVec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>v100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.165436</td>\n",
       "      <td>0.239482</td>\n",
       "      <td>0.227591</td>\n",
       "      <td>0.059136</td>\n",
       "      <td>-0.088518</td>\n",
       "      <td>-0.198773</td>\n",
       "      <td>0.136136</td>\n",
       "      <td>0.154564</td>\n",
       "      <td>-0.170955</td>\n",
       "      <td>-0.266309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045909</td>\n",
       "      <td>0.246582</td>\n",
       "      <td>0.218609</td>\n",
       "      <td>-0.064755</td>\n",
       "      <td>-0.060573</td>\n",
       "      <td>-0.297136</td>\n",
       "      <td>0.115518</td>\n",
       "      <td>0.303364</td>\n",
       "      <td>-0.395736</td>\n",
       "      <td>0.089327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065757</td>\n",
       "      <td>0.414686</td>\n",
       "      <td>-0.008686</td>\n",
       "      <td>0.255243</td>\n",
       "      <td>-0.008914</td>\n",
       "      <td>-0.310071</td>\n",
       "      <td>0.269971</td>\n",
       "      <td>0.158771</td>\n",
       "      <td>0.101829</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169314</td>\n",
       "      <td>0.249471</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>-0.203343</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>-0.526671</td>\n",
       "      <td>0.035886</td>\n",
       "      <td>0.333843</td>\n",
       "      <td>-0.307271</td>\n",
       "      <td>0.055514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148278</td>\n",
       "      <td>0.328567</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>-0.007978</td>\n",
       "      <td>-0.202856</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>-0.029811</td>\n",
       "      <td>-0.054144</td>\n",
       "      <td>-0.129011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046856</td>\n",
       "      <td>0.198922</td>\n",
       "      <td>0.241233</td>\n",
       "      <td>-0.060744</td>\n",
       "      <td>0.233622</td>\n",
       "      <td>-0.289878</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>0.331367</td>\n",
       "      <td>-0.396100</td>\n",
       "      <td>-0.037822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119733</td>\n",
       "      <td>0.337883</td>\n",
       "      <td>0.209933</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>-0.227033</td>\n",
       "      <td>-0.176050</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>-0.022250</td>\n",
       "      <td>-0.175967</td>\n",
       "      <td>0.217267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051933</td>\n",
       "      <td>0.411650</td>\n",
       "      <td>0.320783</td>\n",
       "      <td>-0.102650</td>\n",
       "      <td>0.105333</td>\n",
       "      <td>-0.282683</td>\n",
       "      <td>0.142717</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>-0.406317</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102327</td>\n",
       "      <td>0.495418</td>\n",
       "      <td>0.105991</td>\n",
       "      <td>0.052755</td>\n",
       "      <td>-0.091100</td>\n",
       "      <td>-0.122655</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.083227</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>-0.185927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001836</td>\n",
       "      <td>0.186536</td>\n",
       "      <td>0.352745</td>\n",
       "      <td>-0.090218</td>\n",
       "      <td>0.047673</td>\n",
       "      <td>-0.274909</td>\n",
       "      <td>0.202773</td>\n",
       "      <td>0.320991</td>\n",
       "      <td>-0.369436</td>\n",
       "      <td>0.052345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0  0.165436  0.239482  0.227591  0.059136 -0.088518 -0.198773  0.136136   \n",
       "1  0.065757  0.414686 -0.008686  0.255243 -0.008914 -0.310071  0.269971   \n",
       "2  0.148278  0.328567  0.040078  0.066744 -0.007978 -0.202856  0.140478   \n",
       "3  0.119733  0.337883  0.209933  0.174100 -0.227033 -0.176050  0.272500   \n",
       "4  0.102327  0.495418  0.105991  0.052755 -0.091100 -0.122655  0.006882   \n",
       "\n",
       "         v8        v9       v10  ...       v91       v92       v93       v94  \\\n",
       "0  0.154564 -0.170955 -0.266309  ...  0.045909  0.246582  0.218609 -0.064755   \n",
       "1  0.158771  0.101829 -0.003029  ...  0.169314  0.249471  0.220200 -0.203343   \n",
       "2 -0.029811 -0.054144 -0.129011  ... -0.046856  0.198922  0.241233 -0.060744   \n",
       "3 -0.022250 -0.175967  0.217267  ... -0.051933  0.411650  0.320783 -0.102650   \n",
       "4  0.083227  0.053082 -0.185927  ... -0.001836  0.186536  0.352745 -0.090218   \n",
       "\n",
       "        v95       v96       v97       v98       v99      v100  \n",
       "0 -0.060573 -0.297136  0.115518  0.303364 -0.395736  0.089327  \n",
       "1  0.205729 -0.526671  0.035886  0.333843 -0.307271  0.055514  \n",
       "2  0.233622 -0.289878  0.008967  0.331367 -0.396100 -0.037822  \n",
       "3  0.105333 -0.282683  0.142717  0.144983 -0.406317  0.061300  \n",
       "4  0.047673 -0.274909  0.202773  0.320991 -0.369436  0.052345  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transformer en data frame\n",
    "df = pd.DataFrame(matVec,columns=[\"v\"+str(i+1) for i in range(matVec.shape[1])])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>v100</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.165436</td>\n",
       "      <td>0.239482</td>\n",
       "      <td>0.227591</td>\n",
       "      <td>0.059136</td>\n",
       "      <td>-0.088518</td>\n",
       "      <td>-0.198773</td>\n",
       "      <td>0.136136</td>\n",
       "      <td>0.154564</td>\n",
       "      <td>-0.170955</td>\n",
       "      <td>-0.266309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246582</td>\n",
       "      <td>0.218609</td>\n",
       "      <td>-0.064755</td>\n",
       "      <td>-0.060573</td>\n",
       "      <td>-0.297136</td>\n",
       "      <td>0.115518</td>\n",
       "      <td>0.303364</td>\n",
       "      <td>-0.395736</td>\n",
       "      <td>0.089327</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065757</td>\n",
       "      <td>0.414686</td>\n",
       "      <td>-0.008686</td>\n",
       "      <td>0.255243</td>\n",
       "      <td>-0.008914</td>\n",
       "      <td>-0.310071</td>\n",
       "      <td>0.269971</td>\n",
       "      <td>0.158771</td>\n",
       "      <td>0.101829</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249471</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>-0.203343</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>-0.526671</td>\n",
       "      <td>0.035886</td>\n",
       "      <td>0.333843</td>\n",
       "      <td>-0.307271</td>\n",
       "      <td>0.055514</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148278</td>\n",
       "      <td>0.328567</td>\n",
       "      <td>0.040078</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>-0.007978</td>\n",
       "      <td>-0.202856</td>\n",
       "      <td>0.140478</td>\n",
       "      <td>-0.029811</td>\n",
       "      <td>-0.054144</td>\n",
       "      <td>-0.129011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198922</td>\n",
       "      <td>0.241233</td>\n",
       "      <td>-0.060744</td>\n",
       "      <td>0.233622</td>\n",
       "      <td>-0.289878</td>\n",
       "      <td>0.008967</td>\n",
       "      <td>0.331367</td>\n",
       "      <td>-0.396100</td>\n",
       "      <td>-0.037822</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119733</td>\n",
       "      <td>0.337883</td>\n",
       "      <td>0.209933</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>-0.227033</td>\n",
       "      <td>-0.176050</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>-0.022250</td>\n",
       "      <td>-0.175967</td>\n",
       "      <td>0.217267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411650</td>\n",
       "      <td>0.320783</td>\n",
       "      <td>-0.102650</td>\n",
       "      <td>0.105333</td>\n",
       "      <td>-0.282683</td>\n",
       "      <td>0.142717</td>\n",
       "      <td>0.144983</td>\n",
       "      <td>-0.406317</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102327</td>\n",
       "      <td>0.495418</td>\n",
       "      <td>0.105991</td>\n",
       "      <td>0.052755</td>\n",
       "      <td>-0.091100</td>\n",
       "      <td>-0.122655</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.083227</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>-0.185927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186536</td>\n",
       "      <td>0.352745</td>\n",
       "      <td>-0.090218</td>\n",
       "      <td>0.047673</td>\n",
       "      <td>-0.274909</td>\n",
       "      <td>0.202773</td>\n",
       "      <td>0.320991</td>\n",
       "      <td>-0.369436</td>\n",
       "      <td>0.052345</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v3        v4        v5        v6        v7  \\\n",
       "0  0.165436  0.239482  0.227591  0.059136 -0.088518 -0.198773  0.136136   \n",
       "1  0.065757  0.414686 -0.008686  0.255243 -0.008914 -0.310071  0.269971   \n",
       "2  0.148278  0.328567  0.040078  0.066744 -0.007978 -0.202856  0.140478   \n",
       "3  0.119733  0.337883  0.209933  0.174100 -0.227033 -0.176050  0.272500   \n",
       "4  0.102327  0.495418  0.105991  0.052755 -0.091100 -0.122655  0.006882   \n",
       "\n",
       "         v8        v9       v10  ...       v92       v93       v94       v95  \\\n",
       "0  0.154564 -0.170955 -0.266309  ...  0.246582  0.218609 -0.064755 -0.060573   \n",
       "1  0.158771  0.101829 -0.003029  ...  0.249471  0.220200 -0.203343  0.205729   \n",
       "2 -0.029811 -0.054144 -0.129011  ...  0.198922  0.241233 -0.060744  0.233622   \n",
       "3 -0.022250 -0.175967  0.217267  ...  0.411650  0.320783 -0.102650  0.105333   \n",
       "4  0.083227  0.053082 -0.185927  ...  0.186536  0.352745 -0.090218  0.047673   \n",
       "\n",
       "        v96       v97       v98       v99      v100     label  \n",
       "0 -0.297136  0.115518  0.303364 -0.395736  0.089327  negative  \n",
       "1 -0.526671  0.035886  0.333843 -0.307271  0.055514  negative  \n",
       "2 -0.289878  0.008967  0.331367 -0.396100 -0.037822  negative  \n",
       "3 -0.282683  0.142717  0.144983 -0.406317  0.061300  negative  \n",
       "4 -0.274909  0.202773  0.320991 -0.369436  0.052345  negative  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ajouter la classe\n",
    "df['label'] = df_ok.label\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction et évaluation en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 101)\n",
      "(12000, 101)\n"
     ]
    }
   ],
   "source": [
    "#partition apprentissage test\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTrain, dfTest = train_test_split(df,train_size=0.7,stratify=df.label,random_state=0)\n",
    "print(dfTrain.shape)\n",
    "print(dfTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je mets un échantillonage stratifié pour avoir les mêmes proportion de classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM avec un noyau RBF par défaut\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(dfTrain[dfTrain.columns[:-1]],dfTrain.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "#prédiction en test\n",
    "pred = clf.predict(dfTest[dfTest.columns[:-1]])\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.70      0.70      6000\n",
      "    positive       0.70      0.70      0.70      6000\n",
      "\n",
      "    accuracy                           0.70     12000\n",
      "   macro avg       0.70      0.70      0.70     12000\n",
      "weighted avg       0.70      0.70      0.70     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#évaluation des performances\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(dfTest.label,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déploiement sur un document supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document à traiter\n",
    "blog_avion = \"My next plane ticket has just been booked! I fly in March to the Philippines. I am currently in the process of preparing for my next trip: what are the places I would like to discover, which itinerary to choose and in which cities to sleep, which domestic flights I will book to move between the islands etc. A trip by plane is an action that is anticipated and prepared a minimum. We can also face hazards, and it is important to know how to react. I indicate in this article various and varied tips on the theme of air travel. During the flight, the time may seem long, especially depending on the duration of your trip. Take something to take care of, and what not to be embarrassed (Quies balls can be useful). Also, feel free to get up from time to time if you need to stretch your legs. Be aware that the toilet is unavailable during take-off and landing, take precautions. A tip, always carry a photocopy of your passport, it can help in case of loss or theft that would occur during the trip. We also often talk about flight delay or cancellation. The wait seems endless in these cases, and travelers too often lack information. It is therefore necessary to try to take his evil in patience. Delayed plane, cancelled flight,companies like Indemnflight can help you get compensated. Note that the delay must be more than 3 hours for compensation to be possible. The advantage of going through Indemnflight is that experts take care of the administrative procedures for you, and they know the rights of passengers well. Note that they are paid only when the compensation is validated. You therefore delegate the management of the request, and then pay them in return a part of the compensation. I personally have only 2h30 maximum of delay to my credit, and have never tested the claim. Finally, the loss of luggage is also a thorny subject! If you do not find it at the exit of the plane, go to a counter available at the airport to report it and find a solution. The price of a plane ticket varies a lot depending on many factors. The level of demand, the time of year, the day and time of booking and the booking time before the departure date are very important elements to take into account if you have in mind to make a good deal when booking your flight. To learn more, I advise you to read this article from my travel blog: How to find a cheap plane ticket? Follow these 11 tips! When booking your flight, check that the formalities of entry into the desired country are feasible on time (example: a visa application must be made in advance), and that your identity card and/or passport will still be valid. Note that for many destinations, the passport must often be valid for a period of 6 months from the date of arrival. It is advisable to book flights well in advance. You rarely get a good deal when you buy a plane ticket a few days before departure. And in this case we take the risk of no longer having a place. In the case of my next trip to the Philippines, it is for these reasons that I will soon book the 2 domestic flights. Especially since I am going to travel with the national airline, and I have the impression that the weight of the luggage allowed is less than the 23 kg allowed on my international flight. I must therefore take this into account. Finally, it is necessary to pay attention to the information provided during the reservation. An error on the last name, or a time on the dates can be expensive, there is in this case a risk of paying extra to make this type of change, or even having to cancel your trip. First of all, find out a little in advance about how you will reach the airport (car + parking, bus, train...). When packing your suitcase, try to think of everything you will need on the spot. Also be aware that some products are prohibited in cabin baggage (liquid products greater than 100ml, knives etc.). Check the airline's website for an exhaustive list, and avoid having certain products confiscated. It is advisable to arrive 1h30 to 2h in advance for short or medium haul flight. 2h to 3h are recommended for international flights. This delay varies according to several elements: is your plane ticket ready or are you going to do the formalities at the airport? Do you have luggage to drop off at the counter? How big is the airport and in which area will you board? To help you and go further, discover the guide: practical tips for flying the first time: the steps to follow at the airport. Do you have any other tips to share when flying? Or adventures to share? Feel free to indicate them in the comments.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['next', 'plane', 'ticket', 'booked', 'fly', 'march', 'philippine', 'currently', 'process', 'preparing', 'next', 'trip', 'place', 'would', 'like', 'discover', 'itinerary', 'choose', 'city', 'sleep', 'domestic', 'flight', 'book', 'move', 'island', 'etc', 'trip', 'plane', 'action', 'anticipated', 'prepared', 'minimum', 'also', 'face', 'hazard', 'important', 'know', 'react', 'indicate', 'article', 'various', 'varied', 'tip', 'theme', 'air', 'travel', 'flight', 'time', 'may', 'seem', 'long', 'especially', 'depending', 'duration', 'trip', 'take', 'something', 'take', 'care', 'embarrassed', 'quies', 'ball', 'useful', 'also', 'feel', 'free', 'get', 'time', 'time', 'need', 'stretch', 'leg', 'aware', 'toilet', 'unavailable', 'takeoff', 'landing', 'take', 'precaution', 'tip', 'always', 'carry', 'photocopy', 'passport', 'help', 'case', 'loss', 'theft', 'would', 'occur', 'trip', 'also', 'often', 'talk', 'flight', 'delay', 'cancellation', 'wait', 'seems', 'endless', 'case', 'traveler', 'often', 'lack', 'information', 'therefore', 'necessary', 'try', 'take', 'evil', 'patience', 'delayed', 'plane', 'cancelled', 'flightcompanies', 'like', 'indemnflight', 'help', 'get', 'compensated', 'note', 'delay', 'must', 'hour', 'compensation', 'possible', 'advantage', 'going', 'indemnflight', 'expert', 'take', 'care', 'administrative', 'procedure', 'know', 'right', 'passenger', 'well', 'note', 'paid', 'compensation', 'validated', 'therefore', 'delegate', 'management', 'request', 'pay', 'return', 'part', 'compensation', 'personally', 'maximum', 'delay', 'credit', 'never', 'tested', 'claim', 'finally', 'loss', 'luggage', 'also', 'thorny', 'subject', 'find', 'exit', 'plane', 'counter', 'available', 'airport', 'report', 'find', 'solution', 'price', 'plane', 'ticket', 'varies', 'lot', 'depending', 'many', 'factor', 'level', 'demand', 'time', 'year', 'day', 'time', 'booking', 'booking', 'time', 'departure', 'date', 'important', 'element', 'take', 'account', 'mind', 'make', 'good', 'deal', 'booking', 'flight', 'learn', 'advise', 'read', 'article', 'travel', 'blog', 'find', 'cheap', 'plane', 'ticket', 'follow', 'tip', 'booking', 'flight', 'check', 'formality', 'entry', 'desired', 'country', 'feasible', 'time', 'example', 'visa', 'application', 'must', 'made', 'advance', 'identity', 'card', 'andor', 'passport', 'still', 'valid', 'note', 'many', 'destination', 'passport', 'must', 'often', 'valid', 'period', 'month', 'date', 'arrival', 'advisable', 'book', 'flight', 'well', 'advance', 'rarely', 'get', 'good', 'deal', 'buy', 'plane', 'ticket', 'day', 'departure', 'case', 'take', 'risk', 'longer', 'place', 'case', 'next', 'trip', 'philippine', 'reason', 'soon', 'book', 'domestic', 'flight', 'especially', 'since', 'going', 'travel', 'national', 'airline', 'impression', 'weight', 'luggage', 'allowed', 'allowed', 'international', 'flight', 'must', 'therefore', 'take', 'account', 'finally', 'necessary', 'pay', 'attention', 'information', 'provided', 'reservation', 'error', 'last', 'name', 'time', 'date', 'expensive', 'case', 'risk', 'paying', 'extra', 'make', 'type', 'change', 'even', 'cancel', 'trip', 'first', 'find', 'little', 'advance', 'reach', 'airport', 'car', 'parking', 'bus', 'train', 'packing', 'suitcase', 'try', 'think', 'everything', 'need', 'spot', 'also', 'aware', 'product', 'prohibited', 'cabin', 'baggage', 'liquid', 'product', 'greater', 'knife', 'etc', 'check', 'airline', 'website', 'exhaustive', 'list', 'avoid', 'certain', 'product', 'confiscated', 'advisable', 'arrive', 'advance', 'short', 'medium', 'haul', 'flight', 'recommended', 'international', 'flight', 'delay', 'varies', 'according', 'several', 'element', 'plane', 'ticket', 'ready', 'going', 'formality', 'airport', 'luggage', 'drop', 'counter', 'big', 'airport', 'area', 'board', 'help', 'discover', 'guide', 'practical', 'tip', 'flying', 'first', 'time', 'step', 'follow', 'airport', 'tip', 'share', 'flying', 'adventure', 'share', 'feel', 'free', 'indicate', 'comment']\n"
     ]
    }
   ],
   "source": [
    "# nettoyage\n",
    "my_clean = clean_tweet(blog_avion, ponctuations, mots_vides,lem)\n",
    "print(my_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation du document en vecteur et prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0066924   0.24769519  0.01260582  0.09944025 -0.0945843  -0.12585595\n",
      "  0.10553544  0.06743696  0.12733747 -0.19930025 -0.1025438  -0.31723949\n",
      "  0.23384025 -0.11691544  0.16146203 -0.28021165 -0.07882709 -0.13041165\n",
      "  0.03096835 -0.08232962  0.05536278  0.1366281  -0.21849342  0.15471544\n",
      " -0.28031291 -0.0930243  -0.11302886 -0.24987747  0.21124785  0.48269671\n",
      "  0.34228278 -0.10007367  0.02435519  0.28576127 -0.07939013 -0.03959949\n",
      " -0.1009843   0.04338355  0.15884658 -0.03122405 -0.09728861 -0.01106532\n",
      " -0.23192152  0.14099443 -0.17758    -0.19375215 -0.04172987  0.1744562\n",
      " -0.25844582 -0.00165392  0.03838709  0.16964937  0.06252785  0.0953357\n",
      "  0.07928051  0.10612152 -0.13520861  0.04745443  0.03035924  0.1231081\n",
      " -0.15309443 -0.20351089 -0.05071848  0.09582506  0.53716279 -0.29640557\n",
      " -0.21854937 -0.26774304 -0.1368038  -0.35458279 -0.05029798  0.18931063\n",
      " -0.08680304  0.30487038 -0.26326861 -0.03460506 -0.26305165  0.14672835\n",
      " -0.14239139 -0.14594759 -0.01068709 -0.36625392  0.23897949 -0.08557494\n",
      "  0.21681316  0.53578861  0.06238861  0.17350405  0.05178405  0.00417722\n",
      "  0.10324152  0.14042506  0.28766506  0.03131418  0.04047696 -0.28213646\n",
      "  0.06782709  0.20057696 -0.37689266  0.01720304]\n"
     ]
    }
   ],
   "source": [
    "#avec le modèle pré-entrainé\n",
    "#et avec la fonction ci-dessus\n",
    "my_vec = my_doc_2_vec(my_clean,trained)\n",
    "print(my_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "#prédiction avec le SVM\n",
    "pred_my_doc = clf.predict(my_vec.reshape(1,-1))\n",
    "\n",
    "#le commentaire est de nature positive\n",
    "print(pred_my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarder le modèle word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P7_01_modèlesurmesure_03_embedding_WOR2VEC.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maintenant je l’enregistre dans un fichier\n",
    "joblib.dump(clf, 'P7_01_modèlesurmesure_03_embedding_WOR2VEC.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
